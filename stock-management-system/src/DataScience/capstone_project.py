# -*- coding: utf-8 -*-
"""Capstone Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t6RO7UTf9LEqDkVjDxmtoLUS1Mc4SqC_

# Capstone Project

## Stock Market Time Series based Prediction

**Overview and Problem Statement:** Stock price forecasting is one of the most challenging tasks in financial decision-making because stock prices are inherently noisy and nonstationary and have been observed to have a random-walk characteristic. Accurate stock price predictions can yield significant profits and therefore econometric and statistical approaches including linear/non-linear methods such as autoregressive (AR) models, moving averages (MA), autoregressive moving averages (ARIMA) and artificial neural networks have
been in use. Deep learning is very attractive for stock market prediction at high frequencies as it can extract features from a large set of raw data without relying on prior knowledge of predictors. Deep neural networks can extract additional information from the residuals of the AR models and improve prediction performance in comparison to the AR models. The goal of this project is to train deep learning networks such as Long Short Term Memory (LSTM) and effectively use it in improving stock market analysis and prediction.

**Specific Challenges:**
1. The stock market prediction is a challenging process, mainly because it exhibits a
close to random-walk behaviour in a stock time series
2. Identifying noise vs stable signals
3. Achieving mean square error below 15%
**Data description:**
This dataset is called ‘Huge Stock Market Dataset’ and contains the historical daily prices and volumes of all US stocks and Exchange Trading Funds (ETFs) on the NYSE, NASDAQ and NYSE MKT. Each file titled with the respective 4 letter company abbreviation contains the following data in csv format: Date, Open, High, Low, Close, Volume, OpenInt. The prices have been adjusted for dividends and splits. The reference list of 4 letter company name abbreviations for all the companies in the dataset can be found at https://www.advfn.com/nasdaq/nasdaq.asp

**References:**
1. Marjanovik, 2017: Huge Stock Market Dataset
2. Chong et al., 2017: Deep Learning networks for stock market analysis and prediction
3. Lee et al., 2019: Multimodal Deep Learning for Finance: Integrating and Forecasting
International Stock Markets

### Importing all the necessary packages
"""

import os
import glob
from statsmodels.tsa.seasonal import seasonal_decompose
#from google.colab import drive
from helper_scripts import config,helper
import plotly.graph_objects as go
from keras.layers import Dense, LSTM
from keras.models import Sequential
from sklearn.preprocessing import MinMaxScaler
from keras.preprocessing.sequence import TimeseriesGenerator
from sklearn.metrics import mean_squared_error
from pandas import datetime
from pandas.tseries.offsets import DateOffset
import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller, kpss
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from pandas.plotting import autocorrelation_plot
from pandas.plotting import lag_plot
import seaborn as sns
from matplotlib import pyplot
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from math import sqrt
import warnings
warnings.filterwarnings("ignore")

# import local packages

"""### Load the data from Google drive
Link: https://drive.google.com/file/d/1wegjR2-SBjAp8CklafW6PIZInJg89Z_c/view?usp=sharing
"""

# fetch data from the database

drive.mount('/content/gdrive')

!unzip gdrive/My\ Drive/Colab\ Notebooks/DataSets/Huge\ Stock\ Market\ Dataset.zip

ETF_labels = glob.glob("Data/ETFs/*.txt")
Stock_labels = glob.glob("Data/Stocks/*.txt")

new_style = {'grid': False}
plt.rc('axes', **new_style)
splot = sns.barplot(x=['ETF', 'Stock'], y=[len(ETF_labels), len(Stock_labels)])
for p in splot.patches:
    splot.annotate(p.get_height(),
                   (p.get_x() + p.get_width() / 2., p.get_height()),
                   ha='center', va='center',
                   xytext=(0, -12),
                   textcoords='offset points')
plt.title("Plot to show the count of ETFs and Stocks in the Data")
plt.ylabel("Count")
plt.show()


def read_text_file(file_path):
    with open(file_path, 'r') as f:
        return f.read()


k = 2

Stock = read_text_file(Stock_labels[k]).split()
Stock_new = []
for j in range(len(Stock)):
    Stock_new.append(Stock[j].split(","))

Stock_new_1 = pd.DataFrame(Stock_new)
Stock_df = Stock_new_1.rename(
    columns=Stock_new_1.iloc[0]).drop(Stock_new_1.index[0])
Stock_df['Date'] = pd.to_datetime(Stock_df['Date'], format='%Y-%m-%d')
Stock_df['Open'] = Stock_df['Open'].astype(float)
Stock_df['High'] = Stock_df['High'].astype(float)
Stock_df['Low'] = Stock_df['Low'].astype(float)
Stock_df['Close'] = Stock_df['Close'].astype(float)
Stock_df['Volume'] = Stock_df['Volume'].astype(int)
Stock_df['OpenInt'] = Stock_df['OpenInt'].astype(int)
Stock_df.set_index('Date', inplace=True)

print(Stock_df.dtypes, "\n")
Stock_df.head()

Columns = Stock_df.columns
color = ['blue', 'red', 'green', 'orange', 'brown', 'black']
labels = ['Opening price', 'High price', 'Low price',
          'Closing price', 'Volume traded', 'Open Int']

fig, ax = plt.subplots(3, 2, figsize=(20, 12))
for col in range(len(Columns)):
    Stock_df[Columns[col]].plot(
        grid=False, ax=ax[col // 2, col % 2], color=color[col])
    ax[col // 2, col % 2].set_title("Plot of daily " + labels[col])
plt.tight_layout()

components = seasonal_decompose(
    Stock_df['Close'], model='multiplicative', freq=365, extrapolate_trend='freq')
fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(12, 8), sharex=True)
components.observed.plot(ax=ax1)
ax1.set_ylabel("Observed")
components.trend.plot(ax=ax2)
ax2.set_ylabel("Trend")
components.seasonal.plot(ax=ax3)
ax3.set_ylabel("Seasonal")
components.resid.plot(ax=ax4)
ax4.set_ylabel("Residual")
plt.tight_layout()
plt.show()

pd.DataFrame({'Observed': components.observed, 'Trend': components.trend,
             'Seasonal': components.seasonal, 'Residual': components.resid})

# Correlation matrix


def plotCorrelationMatrix(df, graphWidth):
    filename = Stock_labels[k]
    filename = filename.split("/")[2].split(".txt")[0]
    df = df.dropna('columns')  # drop columns with NaN
    # keep columns where there are more than 1 unique values
    df = df[[col for col in df if df[col].nunique() > 1]]
    if df.shape[1] < 2:
        print(
            f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')
        return
    corr = df.corr()
    plt.figure(num=None, figsize=(graphWidth, graphWidth),
               dpi=80, facecolor='w', edgecolor='k')
    corrMat = plt.matshow(corr, fignum=1)
    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)
    plt.yticks(range(len(corr.columns)), corr.columns)
    plt.gca().xaxis.tick_bottom()
    plt.colorbar(corrMat)
    plt.title(f'Correlation Matrix for {filename}', fontsize=15)
    plt.show()


plotCorrelationMatrix(Stock_df, 10)

# Scatter and density plots


def plotScatterMatrix(df, plotSize, textSize):
    df = df.select_dtypes(include=[np.number])  # keep only numerical columns
    # Remove rows and columns that would lead to df being singular
    df = df.dropna('columns')
    # keep columns where there are more than 1 unique values
    df = df[[col for col in df if df[col].nunique() > 1]]
    columnNames = list(df)
    if len(columnNames) > 10:  # reduce the number of columns for matrix inversion of kernel density plots
        columnNames = columnNames[:10]
    df = df[columnNames]
    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[
                                    plotSize, plotSize], diagonal='kde')
    corrs = df.corr().values
    for i, j in zip(*plt.np.triu_indices_from(ax, k=1)):
        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2),
                          xycoords='axes fraction', ha='center', va='center', size=textSize)
    plt.suptitle('Scatter and Density Plot')
    plt.show()


plotScatterMatrix(Stock_df, 16, 8)

Stock = read_text_file(Stock_labels[k]).split()
Stock_new = []
for j in range(len(Stock)):
    Stock_new.append(Stock[j].split(","))

Stock_new_1 = pd.DataFrame(Stock_new)
Stock_df = Stock_new_1.rename(
    columns=Stock_new_1.iloc[0]).drop(Stock_new_1.index[0])
Stock_df['Date'] = pd.to_datetime(Stock_df['Date'], format='%Y-%m-%d')
Stock_df['Open'] = Stock_df['Open'].astype(float)
Stock_df['High'] = Stock_df['High'].astype(float)
Stock_df['Low'] = Stock_df['Low'].astype(float)
Stock_df['Close'] = Stock_df['Close'].astype(float)
Stock_df['Volume'] = Stock_df['Volume'].astype(int)
Stock_df['OpenInt'] = Stock_df['OpenInt'].astype(int)
Stock_df.drop(columns=['Open', 'High', 'Low',
              'Volume', 'OpenInt'], inplace=True)
Stock_df.rename(columns={'Close': 'Price'})
Stock_df["DayOfWeek"] = pd.to_datetime(Stock_df['Date']).dt.day_name()
Stock_df["Month"] = pd.to_datetime(Stock_df['Date']).dt.month_name()

Count = Stock_df.groupby(['DayOfWeek']).count().drop(columns=['Date', 'Month'])
Sum = Stock_df.groupby(['DayOfWeek']).sum()
Daywise_df = pd.merge(Count, Sum, left_index=True, right_index=True).reindex(
    ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']).rename(columns={'Close_x': 'Count', 'Close_y': 'Sum'})

Count = Stock_df.groupby(['Month']).count().drop(columns=['Date', 'DayOfWeek'])
Sum = Stock_df.groupby(['Month']).sum()
Monthwise_df = pd.merge(Count, Sum, left_index=True, right_index=True).reindex(
    ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']).rename(columns={'Close_x': 'Count', 'Close_y': 'Sum'})

f, ax = plt.subplots(2, 1, figsize=(16, 10))
sns.set_theme(style="whitegrid")
sns.barplot(ax=ax[0], data=Daywise_df, x=Daywise_df.index, y=Daywise_df.Count)
sns.despine()

sns.set_theme(style="whitegrid")
sns.barplot(ax=ax[1], data=Monthwise_df,
            x=Monthwise_df.index, y=Monthwise_df.Count)
plt.xticks(rotation=90)
sns.despine()
plt.tight_layout()
plt.show()

f, ax = plt.subplots(2, 1, figsize=(16, 10))
sns.set_theme(style="whitegrid")
sns.barplot(ax=ax[0], data=Daywise_df, x=Daywise_df.index, y=Daywise_df.Sum)
sns.despine()

sns.set_theme(style="whitegrid")
sns.barplot(ax=ax[1], data=Monthwise_df,
            x=Monthwise_df.index, y=Monthwise_df.Sum)
sns.despine()
plt.tight_layout()
plt.show()

series = Stock_df.set_index('Date').drop(columns=['DayOfWeek', 'Month'])
lag_plot(series)
pyplot.show()

values = pd.DataFrame(series.values)
dataframe = pd.concat([values.shift(1), values], axis=1)
dataframe.columns = ['t-1', 't+1']
result = dataframe.corr()
print(result)

autocorrelation_plot(series)
pyplot.show()

pyplot.figure(figsize=(16, 10))
lags_acf = 210
lags_pacf = 50
# acf
axis = pyplot.subplot(2, 1, 1)
plot_acf(series, ax=axis, lags=lags_acf)
# pacf
axis = pyplot.subplot(2, 1, 2)
plot_pacf(series, ax=axis, lags=lags_pacf)
# show plot
pyplot.show()
plt.tight_layout()

"""### Check the data stationarity using Augmented Dickey-Fuller test

The **Augmented Dickey-Fuller test** is a type of statistical test called a unit root test. The intuition behind a unit root test is that it determines how strongly a time series is defined by a trend.

There are a number of unit root tests and the Augmented Dickey-Fuller is one of the more widely used. It uses an autoregressive model and optimizes an information criterion across multiple different lag values.

The null hypothesis of the test is that the time series can be represented by a unit root, that it is not stationary (has some time-dependent structure). The alternate hypothesis (rejecting the null hypothesis) is that the time series is stationary.

* Null Hypothesis (H0): If failed to be rejected, it suggests the time series has a unit root, meaning it is non-stationary. It has some time dependent structure.

* Alternate Hypothesis (H1): The null hypothesis is rejected; it suggests the time series does not have a unit root, meaning it is stationary. It does not have time-dependent structure.

We interpret this result using the p-value from the test. A p-value below a threshold (such as 5% or 1%) suggests we reject the null hypothesis (stationary), otherwise, a p-value above the threshold suggests we fail to reject the null hypothesis (non-stationary).

* p-value > 0.05: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.

* p-value <= 0.05: Reject the null hypothesis (H0), the data does not have a unit root and is stationary.

Let's use the Augmented Dickey-Fuller test to check the stationarity of the seasonal Industrial production data.
"""

Stock_df.drop(columns=['DayOfWeek', 'Month'], inplace=True)
Stock_df.set_index('Date', inplace=True)

# Applying ADF test
level_of_significance = 0.05

i = 0
Stock_df['Diff(0)'] = Stock_df['Close']
while adfuller(Stock_df['Diff('+str(i)+')'].dropna())[1] > level_of_significance:
    Stock_df['Diff('+str(i+1)+')'] = Stock_df['Diff('+str(i)+')'] - \
        Stock_df['Diff('+str(i)+')'].shift(1)
    i = i + 1
Stock_df['Diff('+str(i)+')'].plot()

"""### ARMA(p, q) model

Here we will grid search to find out the optimal values of (p, q) using the lowest BIC critera
"""


def read_text_file(file_path):
    with open(file_path, 'r') as f:
        return f.read()


k = 1

Stock = read_text_file(Stock_labels[k]).split()
Stock_new = []
for j in range(len(Stock)):
    Stock_new.append(Stock[j].split(","))

Stock_new_1 = pd.DataFrame(Stock_new)
Stock_df = Stock_new_1.rename(
    columns=Stock_new_1.iloc[0]).drop(Stock_new_1.index[0])
Stock_df['Date'] = pd.to_datetime(Stock_df['Date'], format='%Y-%m-%d')
Stock_df['Open'] = Stock_df['Open'].astype(float)
Stock_df['High'] = Stock_df['High'].astype(float)
Stock_df['Low'] = Stock_df['Low'].astype(float)
Stock_df['Close'] = Stock_df['Close'].astype(float)
Stock_df['Volume'] = Stock_df['Volume'].astype(int)
Stock_df['OpenInt'] = Stock_df['OpenInt'].astype(int)
Stock_df.set_index('Date', inplace=True)

Stock_df.reset_index(inplace=True)

# grid search ARMA parameters for time series

# evaluate an ARMA model for a given order (p, q)


def evaluate_arma_model(X, arma_order):
    model = sm.tsa.ARMA(X, order=arma_order)
    model_fit = model.fit()
    return model_fit.bic, model_fit.pvalues.max(), (model_fit.resid**2).sum()

# evaluate combinations of p, q values for an ARMA model


def evaluate_models(dataset, p_values, q_values):
    dataset = dataset.astype('float32')
    best_score, best_cfg = float("inf"), None
    for p in p_values:
        for q in q_values:
            order = (p, q)
            try:
                BIC, p_value, RSS = evaluate_arma_model(dataset, order)
                # if p_value < 0.05 and BIC < best_score:
                if BIC < best_score:
                    best_score, best_cfg = BIC, order
                    print('ARMA%s BIC=%.3f RSS=%.3f' % (order, BIC, RSS))
            except:
                continue
    return best_cfg


# evaluate parameters
p_values = range(0, 5)
q_values = range(0, 5)

warnings.filterwarnings("ignore")
(p_optimal_arma, q_optimal_arma) = evaluate_models(
    Stock_df['Close'], p_values, q_values)
print('Best fit model is ARMA(%s, %s)' % (p_optimal_arma, q_optimal_arma))

Stock_df.reset_index(inplace=True)
f, ax = plt.subplots(figsize=(20, 8))
ax.plot(Stock_df['Close'].dropna(), 'b', label="original", color='blue')
arma = sm.tsa.ARMA(Stock_df['Close'].dropna().values,
                   order=(p_optimal_arma, q_optimal_arma))
arma_fit = arma.fit()
ax.plot(arma_fit.predict(), 'r', label="ARMA (" +
        str(p_optimal_arma)+", "+str(q_optimal_arma)+")", color='red')
Stock_df['Predict'] = arma_fit.predict()
plt.legend()
plt.xlabel("Time (days)")
plt.ylabel("Stock price")
plt.show()

Stock_df.set_index('Date', inplace=True)
Stock_df = Stock_df[['Close', 'Predict']]
extra_dates = [Stock_df.index[-1] + DateOffset(days=d) for d in range(1, 180)]
forecast_df = pd.DataFrame(index=extra_dates, columns=Stock_df.columns)
forecast_df['Predict'] = arma_fit.predict(
    start=Stock_df.shape[0] - 1, end=Stock_df.shape[0] + len(extra_dates) - 2)

# plt.figure(figsize=(17,7))
# plt.plot(Stock_df['Close'], 'green', color='blue', label='Actual Price')
# plt.plot(forecast_df['Predict'], color='red', marker='o', linestyle='dashed', label='Predicted Price')
# plt.title('Stock Price Prediction')
# plt.xlabel('Dates')
# plt.ylabel('Stock Price')
# plt.legend()

model = sm.tsa.ARMA(Stock_df['Close'], order=(p_optimal, q_optimal))
fitted = model.fit(disp=-1)
print(fitted.summary())

# Forecast
fc, se, conf = fitted.forecast(179, alpha=0.05)  # 95% conf

# Make as pandas series
fc_series = pd.Series(fc, index=forecast_df.index)
lower_series = pd.Series(conf[:, 0], index=forecast_df.index)
upper_series = pd.Series(conf[:, 1], index=forecast_df.index)

# Plot
plt.figure(figsize=(12, 5), dpi=100)
plt.plot(Stock_df['Close'], label='Actual Price')
plt.plot(fc_series, label='Forecast ARMA (' +
         str(p_optimal)+', '+str(q_optimal)+')')
plt.fill_between(lower_series.index, lower_series, upper_series,
                 color='k', alpha=.15)
plt.title('Stock Price Prediction using ARMA model')
plt.legend(loc='best', fontsize=8)
plt.xlabel('Time')
plt.ylabel('Stock Price')
plt.show()

"""### ARIMA(p, d, q) model

Here we will grid search to find out the optimal values of (p, q) using the lowest BIC critera

#### Call the data
"""


def read_text_file(file_path):
    with open(file_path, 'r') as f:
        return f.read()


k = 1

Stock = read_text_file(Stock_labels[k]).split()
Stock_new = []
for j in range(len(Stock)):
    Stock_new.append(Stock[j].split(","))

Stock_new_1 = pd.DataFrame(Stock_new)
Stock_df = Stock_new_1.rename(
    columns=Stock_new_1.iloc[0]).drop(Stock_new_1.index[0])
Stock_df['Date'] = pd.to_datetime(Stock_df['Date'], format='%Y-%m-%d')
Stock_df['Open'] = Stock_df['Open'].astype(float)
Stock_df['High'] = Stock_df['High'].astype(float)
Stock_df['Low'] = Stock_df['Low'].astype(float)
Stock_df['Close'] = Stock_df['Close'].astype(float)
Stock_df['Volume'] = Stock_df['Volume'].astype(int)
Stock_df['OpenInt'] = Stock_df['OpenInt'].astype(int)
#Stock_df.set_index('Date', inplace = True)

Stock_df.drop(columns=['Open', 'High', 'Low',
              'Volume', 'OpenInt'], inplace=True)
#Stock_df.reset_index(inplace = False)

"""#### Grid search the model using Python Functions"""

# grid search ARIMA parameters for time series

# evaluate an ARIMA model for a given order (p, d, q)


def evaluate_ARIMA_model(X, ARIMA_order):
    model = sm.tsa.ARIMA(X, order=ARIMA_order)
    model_fit = model.fit()
    return model_fit.bic, model_fit.pvalues.max(), (model_fit.resid**2).sum()

# evaluate combinations of p, q values for an ARIMA model


def evaluate_models(dataset, p_values, d_values, q_values):
    dataset = dataset.astype('float32')
    best_score, best_cfg = float("inf"), None
    for p in p_values:
        for q in q_values:
            for d in d_values:
                order = (p, d, q)
                try:
                    BIC, p_value, RSS = evaluate_ARIMA_model(dataset, order)
                # if p_value < 0.05 and BIC < best_score:
                    if RSS < best_score:
                        best_score, best_cfg = RSS, order
                        print('ARIMA%s BIC=%.3f RSS=%.3f' % (order, BIC, RSS))
                except:
                    continue
    return best_cfg


# evaluate parameters
p_values = range(0, 5)
q_values = range(0, 5)
d_values = range(0, 4)

warnings.filterwarnings("ignore")
(p_optimal_arima, d_optimal_arima, q_optimal_arima) = evaluate_models(
    Stock_df['Close'], p_values, d_values, q_values)
print('Best fit model is ARIMA(%s, %s, %s)' %
      (p_optimal_arima, d_optimal_arima, q_optimal_arima))

Stock_df.set_index('Date', inplace=True)

model = sm.tsa.ARIMA(Stock_df['Close'], order=(
    p_optimal, d_optimal, q_optimal))
fitted = model.fit(disp=-1)
print(fitted.summary())

# Forecast
fc, se, conf = fitted.forecast(179, alpha=0.05)  # 95% conf

# Make as pandas series
fc_series = pd.Series(fc, index=forecast_df.index)
lower_series = pd.Series(conf[:, 0], index=forecast_df.index)
upper_series = pd.Series(conf[:, 1], index=forecast_df.index)

# Plot
plt.figure(figsize=(12, 5), dpi=100)
plt.plot(Stock_df['Close'], label='Actual Price')
plt.plot(fc_series, label='Forecast ARIMA ('+str(p_optimal_arima) +
         ', '+str(d_optimal_arima)+', '+str(q_optimal_arima)+')')
plt.fill_between(lower_series.index, lower_series, upper_series,
                 color='k', alpha=.15)
plt.title('Stock Price Prediction using ARIMA model')
plt.legend(loc='best', fontsize=8)
plt.xlabel('Time')
plt.ylabel('Stock Price')
plt.show()

"""### LSTM
https://colab.research.google.com/drive/1jEDPkZ1-FtP4tAXHK5pGZkA9Jdt2BRpp
"""


def read_text_file(file_path):
    with open(file_path, 'r') as f:
        return f.read()


k = 1

Stock = read_text_file(Stock_labels[k]).split()
Stock_new = []
for j in range(len(Stock)):
    Stock_new.append(Stock[j].split(","))

Stock_new_1 = pd.DataFrame(Stock_new)
Stock_df = Stock_new_1.rename(
    columns=Stock_new_1.iloc[0]).drop(Stock_new_1.index[0])
Stock_df['Date'] = pd.to_datetime(Stock_df['Date'], format='%Y-%m-%d')
Stock_df['Open'] = Stock_df['Open'].astype(float)
Stock_df['High'] = Stock_df['High'].astype(float)
Stock_df['Low'] = Stock_df['Low'].astype(float)
Stock_df['Close'] = Stock_df['Close'].astype(float)
Stock_df['Volume'] = Stock_df['Volume'].astype(int)
Stock_df['OpenInt'] = Stock_df['OpenInt'].astype(int)
#Stock_df.set_index('Date', inplace = True)

Stock_df.drop(columns=['Open', 'High', 'Low',
              'Volume', 'OpenInt'], inplace=True)
#Stock_df.reset_index(inplace = False)

plt.figure(figsize=(16, 6))
plt.title('Close Price History', fontsize=18)
plt.plot(Stock_df['Close'])
plt.xlabel('Time (in Days)', fontsize=18)
plt.ylabel('Price', fontsize=18)
plt.show()

# Create a new dataframe with only the 'Close column
data = Stock_df.filter(['Close'])
# Convert the dataframe to a numpy array
dataset = data.values
# Get the number of rows to train the model on
training_data_len = int(np.ceil(len(dataset) * 0.80))

# Scale the data
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(dataset)

# Create the training data set
# Create the scaled training data set
train_data = scaled_data[0:int(training_data_len), :]
# Split the data into x_train and y_train data sets
x_train = []
y_train = []

for i in range(60, len(train_data)):
    x_train.append(train_data[i-60:i, 0])
    y_train.append(train_data[i, 0])
    if i <= 61:
        print(x_train)
        print(y_train)
        print()

# Convert the x_train and y_train to numpy arrays
x_train, y_train = np.array(x_train), np.array(y_train)

# Reshape the data
x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))
# x_train.shape

# Build the LSTM model
model = Sequential()
model.add(LSTM(128, return_sequences=True, input_shape=(x_train.shape[1], 1)))
model.add(LSTM(64, return_sequences=False))
model.add(Dense(25))
model.add(Dense(1))

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
model.fit(x_train, y_train, batch_size=1, epochs=1)

# Create the testing data set
# Create a new array containing scaled values from index 1543 to 2002
test_data = scaled_data[training_data_len - 60:, :]
# Create the data sets x_test and y_test
x_test = []
y_test = dataset[training_data_len:, :]
for i in range(60, len(test_data)):
    x_test.append(test_data[i-60:i, 0])

# Convert the data to a numpy array
x_test = np.array(x_test)

# Reshape the data
x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))

# Get the models predicted price values
predictions = model.predict(x_test)
predictions = scaler.inverse_transform(predictions)

# Get the root mean squared error (RMSE)
rmse = np.sqrt(np.mean(((predictions - y_test) ** 2)))
rmse

# Plot the data
train = data[:training_data_len]
valid = data[training_data_len:]
valid['Predictions'] = predictions
# Visualize the data
plt.figure(figsize=(16, 6))
plt.title('Model')
plt.xlabel('Date', fontsize=18)
plt.ylabel('Close Price', fontsize=18)
plt.plot(train['Close'], color='blue')
plt.plot(valid['Close'], color='yellow')
plt.plot(valid['Predictions'], linestyle='dashed', color='green')
plt.legend(['Train', 'Val', 'Predictions'], loc='best')
plt.show()

# Show the valid and predicted prices
valid

"""### LSTM"""


def read_text_file(file_path):
    with open(file_path, 'r') as f:
        return f.read()


k = 1

Stock = read_text_file(Stock_labels[k]).split()
Stock_new = []
for j in range(len(Stock)):
    Stock_new.append(Stock[j].split(","))

Stock_new_1 = pd.DataFrame(Stock_new)
Stock_df = Stock_new_1.rename(
    columns=Stock_new_1.iloc[0]).drop(Stock_new_1.index[0])
Stock_df['Date'] = pd.to_datetime(Stock_df['Date'], format='%Y-%m-%d')
Stock_df['Open'] = Stock_df['Open'].astype(float)
Stock_df['High'] = Stock_df['High'].astype(float)
Stock_df['Low'] = Stock_df['Low'].astype(float)
Stock_df['Close'] = Stock_df['Close'].astype(float)
Stock_df['Volume'] = Stock_df['Volume'].astype(int)
Stock_df['OpenInt'] = Stock_df['OpenInt'].astype(int)
#Stock_df.set_index('Date', inplace = True)

Stock_df.drop(columns=['Open', 'High', 'Low',
              'Volume', 'OpenInt'], inplace=True)
#Stock_df.reset_index(inplace = False)

close_data = Stock_df['Close'].values
close_data = close_data.reshape((-1, 1))

split_percent = 0.80
split = int(split_percent*len(close_data))

close_train = close_data[:split]
close_test = close_data[split:]

date_train = Stock_df['Date'][:split]
date_test = Stock_df['Date'][split:]

print(len(close_train))
print(len(close_test))

look_back = 15

train_generator = TimeseriesGenerator(
    close_train, close_train, length=look_back, batch_size=20)
test_generator = TimeseriesGenerator(
    close_test, close_test, length=look_back, batch_size=1)

# model = Sequential()
# model.add(LSTM(128, return_sequences=True, input_shape= (x_train.shape[1], 1)))
# model.add(LSTM(64, return_sequences=False))
# model.add(Dense(25))
# model.add(Dense(1))

# # Compile the model
# model.compile(optimizer='adam', loss='mean_squared_error')

# # Train the model
# model.fit(x_train, y_train, batch_size=1, epochs=1)


model = Sequential()
model.add(LSTM(128, return_sequences=True,
          activation='relu', input_shape=(look_back, 1)))
model.add(LSTM(64, return_sequences=False))
model.add(Dense(25))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')

num_epochs = 25
model.fit_generator(train_generator, epochs=num_epochs, verbose=1)

prediction = model.predict_generator(test_generator)

close_train = close_train.reshape((-1))
close_test = close_test.reshape((-1))
prediction = prediction.reshape((-1))

trace1 = go.Scatter(
    x=date_train,
    y=close_train,
    mode='lines',
    name='Train'
)
trace2 = go.Scatter(
    x=date_test,
    y=prediction,
    mode='lines',
    name='Prediction'
)
trace3 = go.Scatter(
    x=date_test,
    y=close_test,
    mode='lines',
    name='Test'
)
layout = go.Layout(
    title="Google Stock",
    xaxis={'title': "Date"},
    yaxis={'title': "Close Price"}
)
fig = go.Figure(data=[trace1, trace2, trace3], layout=layout)
fig.show()

close_data = close_data.reshape((-1))


def predict(num_prediction, model):
    prediction_list = close_data[-look_back:]

    for _ in range(num_prediction):
        x = prediction_list[-look_back:]
        x = x.reshape((1, look_back, 1))
        out = model.predict(x)[0][0]
        prediction_list = np.append(prediction_list, out)
    prediction_list = prediction_list[look_back-1:]

    return prediction_list


def predict_dates(num_prediction):
    last_date = Stock_df['Date'].values[-1]
    prediction_dates = pd.date_range(
        last_date, periods=num_prediction+1).tolist()
    return prediction_dates


num_prediction = 180
forecast = predict(num_prediction, model)
forecast_dates = predict_dates(num_prediction)
forecast_df = pd.DataFrame({'Date': forecast_dates, 'Prediction': forecast})

trace1 = go.Scatter(
    x=Stock_df['Date'],
    y=Stock_df['Close'],
    mode='lines',
    name='Actual'
)
trace2 = go.Scatter(
    x=forecast_df['Date'],
    y=forecast_df['Prediction'],
    mode='lines',
    name='Prediction'
)
layout = go.Layout(
    title="Google Stock",
    xaxis={'title': "Date"},
    yaxis={'title': "Close Price"}
)
fig = go.Figure(data=[trace1, trace2], layout=layout)
fig.show()

forecast_df

forecast

forecast_dates

"""https://machinelearningmastery.com/autoregression-models-time-series-forecasting-python/

https://machinelearningmastery.com/how-to-develop-an-autoregression-forecast-model-for-household-electricity-consumption/

**Youtube Links**
1. https://www.youtube.com/watch?v=5-2C4eO4cPQ&ab_channel=ritvikmath
2. https://www.youtube.com/watch?v=DeORzP0go5I&ab_channel=ritvikmath
3. https://www.youtube.com/watch?v=3UmyHed0iYE&ab_channel=ritvikmath
4. https://www.youtube.com/watch?v=-vSzKfqcTDg&ab_channel=AnalyticsUniversity
"""
